---
title: "Chapter 2"
author: "Cory Lanker"
date: "11/4/2019"
output: html_document
---

```{r setup, include=FALSE}
library(MASS)
library(mvtnorm)
library(tidyverse)
library(RobustGaSP)
library(laGP)
knitr::opts_chunk$set(echo = TRUE)
```

## Example 1: 3 points, linear regression coef. estimation
```{r}
Sp <- diag(2)
Sp_inv <- solve(Sp)
X <- matrix(c(1, 1, 1, -5, 2, 5), nrow = 2, byrow = T)
y <- as.vector(c(-5.44, 0.44, 4.56))
sn2 <- 1
A <- sn2 * (X %*% t(X)) + Sp_inv
A_inv <- solve(A)
w_bar <- (1 / sn2) * (A_inv %*% X) %*% y
```

```{r}
out <- expand.grid(x1 = seq(0, 1, 0.05), x2 = seq(3, 5, 0.1))
z <- 1:nrow(out)
out %>%
  ggplot(aes(x = x1, y = x2)) +
  geom_point(aes(col = z))
```

```{r}
# Prior ------------------
out <- expand.grid(x1 = seq(-2, 2, 0.01), x2 = seq(-2, 2, 0.01))
z1 <- dmvnorm(out, rep(0, 2), Sp)
z2 <- dmvnorm(out, w_bar, A_inv)
out %>%
  ggplot(aes(x = x1, y = x2)) +
  geom_contour(aes(z = z1), col = 'blue') +
  geom_contour(aes(z = z2), col = 'red')
```

```{r}
## Prediction
xs <- as.matrix(expand.grid(1, seq(-6, 6, 0.01)))
ys <- tibble(m = as.double()) %>% mutate(std = m)
for (i in 1:nrow(xs))
{
  ys[i, 1] <- (1 / sn2) * (xs[i, ] %*% (A_inv %*% (X %*% y)))
  ys[i, 2] <- sqrt(xs[i, , drop = F] %*% (A_inv %*% t(xs[i, , drop = F])))
}
```

```{r}
dat <- tibble(x = X[2, ], y = y)
ys %>%
  ggplot(aes(x = xs[, 2])) +
    geom_line(aes(y = m), col = 'black') +
    geom_line(aes(y = m + 2 * std), col = 'blue') +
    geom_line(aes(y = m - 2 * std), col = 'blue') +
    coord_fixed(xlim = c(-5, 5), ylim = c(-5.2, 4.8)) +
    geom_point(data = dat, aes(x = x, y = y), pch = 'x', size = 5, col = 'red')
```

## Example 2: 5 points in Figure 2.2 plot
```{r}
## data --------------
y <- as.vector(c(-2, 0, 1, 2, -1))
x <- as.vector(c(-4, -3, -1, 0, 2))
dat <- tibble(x = x, y = y)
## GP ----------------
K <- exp(-0.5 * (rdist(x) ^ 2)) 
K_inv <- solve(K)
xs <- seq(-5, 5, 0.05)
ys <- tibble(x = xs, ym = NA, ysd = NA)
for (i in 1:length(xs))
{
  Ks <- exp(-0.5 * (rdist(xs[i], x) ^ 2)) 
  ys[i, 2] <- Ks %*% (K_inv %*% y)  
  ys[i, 3] <- sqrt(1 - Ks %*% (K_inv %*% t(Ks)))
}

ys %>%
  ggplot(aes(x = xs)) +
  geom_line(aes(y = ym), col = 'green') +
  geom_line(aes(y = ym - 2 * ysd), col = 'red') +
  geom_line(aes(y = ym + 2 * ysd), col = 'black') +
  geom_point(data = dat, aes(x = x, y = y), pch = '+', col = 'blue', size = 6)
```

## R package RobustGaSP
```{r}
## Example 1
#X <- matrix(c(1, 1, 1, -5, 2, 5), nrow = 2, byrow = T) # Note: column vectors for GPfit
#X <- matrix(c(1, 1, 1, -5, 2, 5), ncol = 2) # Note: ... and don't need the ones column
X <- as.matrix(c(-5, 2, 5), ncol = 1) # Note: ... and don't need the ones column
y <- as.matrix(c(-5.44, 0.44, 4.56), ncol = 1)
GPmodel <- rgasp(X, y)
plot(GPmodel)
show(GPmodel)
xs <- as.matrix(seq(-6, 6, , 1000))
pred <- predict(GPmodel, xs, )
plot(xs, pred$mean, type = 'l')
lines(xs, pred$lower95, col=2)
lines(xs, pred$upper95, col=2)
lines(xs, pred$mean + 2 * pred$sd, col=3, lty=2)
lines(xs, pred$mean - 2 * pred$sd, col=3, lty=2)
```


```{r}
  #------------------------
  # a 3 dimensional example
  #------------------------
  # dimensional of the inputs
  dim_inputs <- 3    
  # number of the inputs
  num_obs <- 30       
  # uniform samples of design
  input <- matrix(runif(num_obs*dim_inputs), num_obs,dim_inputs) 
  
  # Following codes use maximin Latin Hypercube Design, which is typically better than uniform
  # library(lhs)
  # input <- maximinLHS(n=num_obs, k=dim_inputs)  ##maximin lhd sample
  
  # outputs from the 3 dim dettepepel.3.data function
  
  output = matrix(0,num_obs,1)
  for(i in 1:num_obs){
    output[i]<-dettepepel.3.data (input[i,])
  }
  
  # use constant mean basis, with no constraint on optimization
  m1<- rgasp(design = input, response = output, lower_bound=FALSE)
  
  # the following use constraints on optimization
  # m1<- rgasp(design = input, response = output, lower_bound=TRUE)
  
  # the following use a single start on optimization
  # m1<- rgasp(design = input, response = output, lower_bound=FALSE)
  
  # number of points to be predicted 
  num_testing_input <- 5000    
  # generate points to be predicted
  testing_input <- matrix(runif(num_testing_input*dim_inputs),num_testing_input,dim_inputs)
  # Perform prediction
  m1.predict<-predict(m1, testing_input)
  # Predictive mean
  m1.predict$mean  
  
  # The following tests how good the prediction is 
  testing_output <- matrix(0,num_testing_input,1)
  for(i in 1:num_testing_input){
    testing_output[i]<-dettepepel.3.data(testing_input[i,])
  }
  
  # compute the MSE, average coverage and average length
  # out of sample MSE
  MSE_emulator <- sum((m1.predict$mean-testing_output)^2)/(num_testing_input)  
  
  # proportion covered by 95% posterior predictive credible interval
  prop_emulator <- length(which((m1.predict$lower95<=testing_output)
                   &(m1.predict$upper95>=testing_output)))/num_testing_input
  
  # average length of  posterior predictive credible interval
  length_emulator <- sum(m1.predict$upper95-m1.predict$lower95)/num_testing_input
  
  # output of prediction
  MSE_emulator
  prop_emulator
  length_emulator  
  # normalized RMSE
  sqrt(MSE_emulator/mean((testing_output-mean(output))^2 ))


  #-----------------------------------
  # a 2 dimensional example with trend
  #-----------------------------------
  # dimensional of the inputs
  dim_inputs <- 2    
  # number of the inputs
  num_obs <- 20       
  
  # uniform samples of design
  input <-matrix(runif(num_obs*dim_inputs), num_obs,dim_inputs) 
  # Following codes use maximin Latin Hypercube Design, which is typically better than uniform
  # library(lhs)
  # input <- maximinLHS(n=num_obs, k=dim_inputs)  ##maximin lhd sample
  
  # outputs from the 2 dim Brainin function
  
  output <- matrix(0,num_obs,1)
  for(i in 1:num_obs){
    output[i]<-limetal.2.data (input[i,])
  }
  
  #mean basis (trend)
  X<-cbind(rep(1,num_obs), input )
  
  
  # use constant mean basis with trend, with no constraint on optimization
  m2<- rgasp(design = input, response = output,trend =X,  lower_bound=FALSE)
  
  
  # number of points to be predicted 
  num_testing_input <- 5000    
  # generate points to be predicted
  testing_input <- matrix(runif(num_testing_input*dim_inputs),num_testing_input,dim_inputs)
  
  # trend of testing
  testing_X<-cbind(rep(1,num_testing_input), testing_input )
  
  
  # Perform prediction
  m2.predict<-predict(m2, testing_input,testing_trend=testing_X)
  # Predictive mean
  #m2.predict$mean  
  
  # The following tests how good the prediction is 
  testing_output <- matrix(0,num_testing_input,1)
  for(i in 1:num_testing_input){
    testing_output[i]<-limetal.2.data(testing_input[i,])
  }
  
  # compute the MSE, average coverage and average length
  # out of sample MSE
  MSE_emulator <- sum((m2.predict$mean-testing_output)^2)/(num_testing_input)  
  
  # proportion covered by 95% posterior predictive credible interval
  prop_emulator <- length(which((m2.predict$lower95<=testing_output)
                   &(m2.predict$upper95>=testing_output)))/num_testing_input
  
  # average length of  posterior predictive credible interval
  length_emulator <- sum(m2.predict$upper95-m2.predict$lower95)/num_testing_input
  
  # output of prediction
  MSE_emulator
  prop_emulator
  length_emulator  
  # normalized RMSE
  sqrt(MSE_emulator/mean((testing_output-mean(output))^2 ))


  #--------------------------------------------------------------------------------------
  # an 8 dimensional example using only a subset inputs and a noise with unknown variance
  #--------------------------------------------------------------------------------------
  # dimensional of the inputs
  dim_inputs <- 8    
  # number of the inputs
  num_obs <- 30       
  
  # uniform samples of design
  input <-matrix(runif(num_obs*dim_inputs), num_obs,dim_inputs) 
  # Following codes use maximin Latin Hypercube Design, which is typically better than uniform
  # library(lhs)
  # input <- maximinLHS(n=num_obs, k=dim_inputs)  # maximin lhd sample
  
  # rescale the design to the domain
  input[,1]<-0.05+(0.15-0.05)*input[,1];
  input[,2]<-100+(50000-100)*input[,2];
  input[,3]<-63070+(115600-63070)*input[,3];
  input[,4]<-990+(1110-990)*input[,4];
  input[,5]<-63.1+(116-63.1)*input[,5];
  input[,6]<-700+(820-700)*input[,6];
  input[,7]<-1120+(1680-1120)*input[,7];
  input[,8]<-9855+(12045-9855)*input[,8];
  
  # outputs from the 8 dim Borehole function
  
  output=matrix(0,num_obs,1)
  for(i in 1:num_obs){
    output[i]=borehole(input[i,])
  }
  
  
    
    
  
  # use constant mean basis with trend, with no constraint on optimization
  m3<- rgasp(design = input[,c(1,4,6,7,8)], response = output,
             nugget.est=TRUE, lower_bound=FALSE)
  
  
  # number of points to be predicted 
  num_testing_input <- 5000    
  # generate points to be predicted
  testing_input <- matrix(runif(num_testing_input*dim_inputs),num_testing_input,dim_inputs)
  
  # resale the points to the region to be predict
  testing_input[,1]<-0.05+(0.15-0.05)*testing_input[,1];
  testing_input[,2]<-100+(50000-100)*testing_input[,2];
  testing_input[,3]<-63070+(115600-63070)*testing_input[,3];
  testing_input[,4]<-990+(1110-990)*testing_input[,4];
  testing_input[,5]<-63.1+(116-63.1)*testing_input[,5];
  testing_input[,6]<-700+(820-700)*testing_input[,6];
  testing_input[,7]<-1120+(1680-1120)*testing_input[,7];
  testing_input[,8]<-9855+(12045-9855)*testing_input[,8];
  
  
  # Perform prediction
  m3.predict<-predict(m3, testing_input[,c(1,4,6,7,8)])
  # Predictive mean
  #m3.predict$mean  
  
  # The following tests how good the prediction is 
  testing_output <- matrix(0,num_testing_input,1)
  for(i in 1:num_testing_input){
    testing_output[i]<-borehole(testing_input[i,])
  }
  
  # compute the MSE, average coverage and average length
  # out of sample MSE
  MSE_emulator <- sum((m3.predict$mean-testing_output)^2)/(num_testing_input)  
  
  # proportion covered by 95% posterior predictive credible interval
  prop_emulator <- length(which((m3.predict$lower95<=testing_output)
                   &(m3.predict$upper95>=testing_output)))/num_testing_input
  
  # average length of  posterior predictive credible interval
  length_emulator <- sum(m3.predict$upper95-m3.predict$lower95)/num_testing_input
  
  # output of sample prediction
  MSE_emulator
  prop_emulator
  length_emulator  
  # normalized RMSE
  sqrt(MSE_emulator/mean((testing_output-mean(output))^2 ))

```






